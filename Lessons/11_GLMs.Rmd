---
title: "11: Generalized Linear Models"
author: "Environmental Data Analytics | Kateri Salk"
date: "Spring 2019"
output: pdf_document
geometry: margin=2.54cm
editor_options: 
  chunk_output_type: console
---

## LESSON OBJECTIVES
1. Describe the components of the generalized linear model (GLM)
2. Apply special cases of the GLM to real datasets
3. Interpret and report the results of GLMs in publication-style formats

## SET UP YOUR DATA ANALYSIS SESSION

```{r, message = FALSE, warning = FALSE}
getwd()
library(tidyverse)

PeterPaul.nutrients <- read.csv("./Data/Processed/NTL-LTER_Lake_Nutrients_PeterPaul_Processed.csv")
EPAair <- read.csv("./Data/Processed/EPAair_O3PM25_3sites1718_processed.csv")

# Set date to date format
EPAair$Date <- as.Date(EPAair$Date, format = "%Y-%m-%d")
PeterPaul.nutrients$sampledate <- as.Date(PeterPaul.nutrients$sampledate, format = "%Y-%m-%d")

# remove negative values for depth_id
PeterPaul.nutrients <- filter(PeterPaul.nutrients, depth_id > 0)

# set depth_id to factor
PeterPaul.nutrients$depth_id <- as.factor(PeterPaul.nutrients$depth_id)

mytheme <- theme_classic(base_size = 14) +
  theme(axis.text = element_text(color = "black"), 
        legend.position = "top")
theme_set(mytheme)

```

## GENERALIZED LINEAR MODELS 

The one-sample test (model of the mean), two-sample t-test, analysis of variance (ANOVA), and linear regression are all special cases of the **generalized linear model** (GLM). The GLM also includes analyses not covered in this class, including logistic regression, multinomial regression, chi square, and log-linear models. The common characteristic of general linear models is the expression of a continuous response variable as a linear combination of the effects of categorical or continuous explanatory variables, plus an error term that expresses the random error associated with the coefficients of all explanatory variables. The explanatory variables comprise the deterministic component of the model, and the error term comprises the stochastic component of the model. Historically, artificial distinctions were made between linear models that contained categorical and continuous explanatory variables, but this distinction is no longer made. The inclusion of these models within the umbrella of the GLM allows models to fit the main effects of both categorical and continuous explanatory variables as well as their interactions. 

### Choosing a model from your data: A "cheat sheet"

**T-test:** Continuous response, one categorical explanatory variable with two categories (or comparison to a single value if a one-sample test).

**One-way ANOVA (Analysis of Variance):** Continuous response, one categorical explanatory variable with more than two categories.

**Two-way ANOVA (Analysis of Variance)** Continuous response, two categorical explanatory variables.

**Single Linear Regression** Continuous response, one continuous explanatory variable.

**Multiple Linear Regression** Continuous response, two or more continuous explanatory variables.

**ANCOVA (Analysis of Covariance)** Continuous response, categorical explanatory variable(s) and  continuous explanatory variable(s).

If multiple explanatory variables are chosen, they may be analyzed with respect to their **main effects** on the model (i.e., their separate impacts on the variance explained) or with respsect to their **interaction effects,** the effect of interacting explanatory variables on the model. 

### Assumptions of the GLM

The GLM is based on the assumption that the data approximate a normal distribution (or a linearly transformed normal distribution). We will discuss the non-parametric analogues to several of these tests if the assumptions of normality are violated. For tests that analyze categorical explanatory variables, the assumption is that the variance in the response variable is equal among groups. Note: environmental data often violate the assumptions of normality and equal variance, and we will often proceed with a GLM even if these assumptions are violated. In this situation, you must justify your decision. 

## T-TEST AND ONE-WAY ANOVA
### One-sample t-test
The object of a one sample test is to test the null hypothesis that the mean of the group is equal to a specific value. For example, we might ask ourselves (from the EPA air quality processed dataset): 

Are Ozone levels below the threshold for "good" AQI index (0-50)?

```{r}
summary(EPAair$Ozone)

# Evaluate assumption of normal distribution
shapiro.test(EPAair$Ozone) #The p-value is really low, meaning that we reject the hypothesis of normal distribution
ggplot(EPAair, aes(x = Ozone)) +
  geom_histogram(stat = "count")
qqnorm(EPAair$Ozone); qqline(EPAair$Ozone)
#It is normal for environmental dataset to have long right hand tail
#And in this case, other than the long right hand tail, the data more or less still looks normal
O3.onesample <- t.test(EPAair$Ozone, mu = 50, alternative = "less") #If not specifying alternative, t.test will just see whether the data is different from mu. It will not specifically check whether the data is more or less than mu.
O3.onesample
```

What information does the output give us? How might we report this information in a report?

> ANWSER: Ozone AQI values in 2017-2018 were significantly lower than 50 (t=-41.9, df=1084, p<0.0001)

### Two-sample t-test
The two-sample *t* test is used to test the hypothesis that the mean of two samples is equivalent. Unlike the one-sample tests, a two-sample test requires a second assumption that the variance of the two groups is equivalent. Are Ozone levels different between Blackstone and Bryson City?

```{r}
shapiro.test(EPAair$Ozone[EPAair$Site.Name == "Blackstone"]) 
shapiro.test(EPAair$Ozone[EPAair$Site.Name == "Bryson City"])
#Both cities do not have normal distribution
var.test(EPAair$Ozone ~ EPAair$Site.Name) #One city has larger variance than other

ggplot(EPAair, aes(x = Ozone, color = Site.Name)) +
  geom_freqpoly(stat = "count")
#In this case it is safer to go by non-parametric version

# Format as a t-test
O3.twosample <- t.test(EPAair$Ozone ~ EPAair$Site.Name)
O3.twosample
O3.twosample$p.value

# Format as a GLM
O3.twosample2 <- lm(EPAair$Ozone ~ EPAair$Site.Name)
summary(O3.twosample2) #The intercept is the first thing to be selected
#So the estimate coefficient for blackstone is 38.48, while bryson city is 3.299 less than blackstone, which is the same with the mean shown in t-test
```
### Non-parametric equivalent of t-test: Wilcoxon test

When we wish to avoid the assumption of normality, we can apply *distribution-free*, or non-parametric, methods in the form of the Wilcoxon rank sum (Mann-Whitney) test. The Wilcoxon test replaces the data by their rank and calculates the sum of the ranks for each group. Notice that the output of the Wilcoxon test is more limited than its parametric equivalent.

```{r}
O3.onesample.wilcox <- wilcox.test(EPAair$Ozone, mu = 50, alternative = "less")
O3.onesample.wilcox #doesnt show the true mean of the population
O3.twosample.wilcox <- wilcox.test(EPAair$Ozone ~ EPAair$Site.Name)
O3.twosample.wilcox
```

### One-way ANOVA
A one-way ANOVA is the same test in practice as a two-sample t-test but for three or more groups. In R, we can  run the model with the function `lm` or `aov`, the latter of which which will allow us to run post-hoc tests to determine pairwise differences.

Are PM2.5 levels different between Blackstone, Bryson City, and Triple Oak?
```{r}
shapiro.test(EPAair$PM2.5[EPAair$Site.Name == "Blackstone"])
shapiro.test(EPAair$PM2.5[EPAair$Site.Name == "Bryson City"])
shapiro.test(EPAair$PM2.5[EPAair$Site.Name == "Triple Oak"])

ggplot(EPAair, aes(x = PM2.5, color = Site.Name)) +
  geom_freqpoly(stat = "count")
qqnorm(EPAair$PM2.5); qqline(EPAair$PM2.5)

bartlett.test(EPAair$PM2.5 ~ EPAair$Site.Name) #test equal variance of more than two groups
#the p-value of 0.08 fail to reject the hypothesis that they have homogenous variance
#reached assumption of equal variance

# Format as a GLM
PM2.5.anova <- lm(EPAair$PM2.5 ~ EPAair$Site.Name)
summary(PM2.5.anova)
#In terms of linear equation pattern, it goes by y=36.7(Blackstone) - 4.266 (Bryson) - 3.25 (Triple).
#Triple oak is 3.25 less than blackstone, not (4.266+3.25)

# Format as an aov
PM2.5.anova2 <- aov(EPAair$PM2.5 ~ EPAair$Site.Name)
summary(PM2.5.anova2)

# Run a post-hoc test for pairwise differences
TukeyHSD(PM2.5.anova2) #Triple Oak and Bryson City are not significantly different from each other's mean
#LM shows that each city is significantly different from intercept when predicting PM2.5
#But it does not show pairwise different, just that each of bryson and triple are significantly different
#from intercept(Blackstone)
#Anova results showed that site.names are significant predictor for PM2.5
#and Tukey showed that among the site.names, bryson and triple are not significantly different from each other, but other two pairs are significantly different from each other

# Plot the results
# How might you edit this graph to make it attractive?
# How might you illustrate significant differences?
PM2.5.anova.plot <- ggplot(EPAair, aes(x = Site.Name, y = PM2.5)) +
  geom_violin(draw_quantiles = 0.5)
print(PM2.5.anova.plot)
```
What information does the output give us? How might we report this information in a report?

> ANSWER: Blackstone has higher concentrations of PM2.5 compared to Bryson and Triple oak (ANOVA; F=16.16, df=1898, p<0.0001)

### Non-parametric equivalent of ANOVA: Kruskal-Wallis Test
As with the Wilcoxon test, the Kruskal-Wallis test is the non-parametric counterpart to the one-way ANOVA. Here, the data from two or more independent samples are replaced with their ranks without regard to the grouping AND based on the between-group sum of squares calculations. 

For multiple comparisons, a p-value < 0.05 indicates that there is a significant difference between groups, but it does not indicate which groups, or in this case, months, differ from each other.

To analyze specific pairs in the data, you must use a *post hoc* test. These include the Dunn's test, a pairwise Mann-Whitney with the Bonferroni correction, or the Conover-Iman test.

```{r}
PM2.5.kw <- kruskal.test(EPAair$PM2.5 ~ EPAair$Site.Name)
PM2.5.kw #does not show post hoc unless we install the dunn.test or FSA package below

# There are two functions to run the Dunn Test
# dunn.test(EPAair$PM2.5, EPAair$Site.Name, kw = T, 
#           table = F, list = T, method = "holm", altp = T)   #From package dunn.test
# dunnTest(EPAair$PM2.5, EPAair$Site.Name)                    #From package FSA
```

## TWO-WAY ANOVA
### Main effects
A two-way ANOVA allows us to examine the effects of two categorical explanatory variables on a continuous response variable. Let's look at the NTL-LTER nutrient dataset for Peter and Paul lakes. What if we wanted to know if total nitrogen concentrations differed based on lake and depth? 

In terms of putting this into y=mx+b format, it will be 
y = alpha(a1) + alpha (a2) + alpha(b1)+alpha(b2) +...+error

```{r}
TNanova.main <- lm(PeterPaul.nutrients$tn_ug ~ PeterPaul.nutrients$lakename + PeterPaul.nutrients$depth_id)
summary(TNanova.main)
#In format of y=mx+b
#Total N concentration = 309(Paul_Lake, depth ID 1) + 105 (Paul_Lake, depth ID 1) + 97 (depth ID 2)  
#After teasing out the effect of both lakes at depth ID 1, the further coefficients do not further divide the effect of other 6 depth ids

TNanova.main2 <- aov(PeterPaul.nutrients$tn_ug ~ PeterPaul.nutrients$lakename + PeterPaul.nutrients$depth_id)
summary(TNanova.main2)
#Anova test shows that lake name and depth ID are significantly
TukeyHSD(TNanova.main2)

# Plot the results
# How might you edit this graph to make it attractive?
# How might you illustrate significant differences?
TNanova.plot <- ggplot(PeterPaul.nutrients, aes(x = lakename, y = tn_ug, color = depth_id)) +
  geom_boxplot()
print(TNanova.plot)
```

### Interaction effects
We may expect the effects of lake and depth to be dependent on each other. For instance, since depth_id is standardized across lakes, the concentrations at each depth_id might depend on which lake is sampled. In this case, we might choose to run an interaction effects two-way ANOVA, which will examine the individual effects of the explanatory variables as well as the interaction of the explanatory variables.

The output gives test statistics for each explanatory variable as well as the interaction effect of the explanatory variables. If the p-value for the interaction effect is less than 0.05, then we would consider the interaction among the explanatory variables to be significant.

```{r}
TNanova.interaction <- aov(PeterPaul.nutrients$tn_ug ~ PeterPaul.nutrients$lakename * PeterPaul.nutrients$depth_id)
summary(TNanova.interaction)

```

If the interaction is significant, we interpret pairwise differences for the interaction. If the interaction is not significant, we interpret differences for the main effects only.
```{r}
TukeyHSD(TNanova.interaction)
```

Pairs are considered to be in the same grouping if the p-value for that pairing is > 0.05. It is easy to see that this grouping process can become complicated when many factors are present for each variable! For a challenge, try writing code that will generate groupings for each factor level in the dataset using the `glht` function in the `multcomp` package. 

### Exercise

Run the same tests and visualizations (main and interaction effects two-way ANOVA) for total phosphorus concentrations. How do your results compare for the different nutrients?

```{r}

```


